[2024-03-11 23:32:25,829] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-11 23:32:29,751] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-03-11 23:32:29,751] [INFO] [runner.py:571:main] cmd = /home/yamanishi/.pyenv/versions/miniconda3-latest/envs/llava/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=20004 --enable_each_rank_log=None llava/train/train_mem.py --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 --deepspeed ./scripts/zero3.json --model_name_or_path lmsys/vicuna-13b-v1.5 --version v1 --data_path ./playground/data/v5/train_conv.json --image_folder /home/yamanishi/project/trip_recommend/data/jalan_image_with_caption --vision_tower openai/clip-vit-large-patch14-336 --pretrain_mm_mlp_adapter ./checkpoints/llava-v1.5-mlp2x-336px-pretrain-vicuna-13b-v1.5/mm_projector.bin --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir ./checkpoints/llava-v1.5-13b-jalan-review-lora-v5 --num_train_epochs 1 --per_device_train_batch_size 8 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy steps --save_steps 50000 --save_total_limit 1 --learning_rate 2e-4 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb --use_poi_token True
[2024-03-11 23:32:31,771] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-11 23:32:33,628] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-03-11 23:32:33,628] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-03-11 23:32:33,628] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-03-11 23:32:33,628] [INFO] [launch.py:163:main] dist_world_size=4
[2024-03-11 23:32:33,628] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-03-11 23:32:37,427] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-11 23:32:37,448] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-11 23:32:37,457] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-11 23:32:37,460] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-11 23:32:39,699] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-11 23:32:39,700] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-11 23:32:39,700] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-11 23:32:39,700] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-03-11 23:32:39,701] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-11 23:32:43,861] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 364, num_elems = 13.27B
Adding LoRA adapters...
[2024-03-11 23:33:07,055] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 755, num_elems = 13.58B
build projector config build projector config LlavaConfig {
  "_name_or_path": "lmsys/vicuna-13b-v1.5",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "down_sample": false,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "mm_hidden_size": 1024,
  "mm_patch_merge_type": "flat",
  "mm_projector_type": "mlp2x_gelu",
  "mm_vision_select_feature": "patch",
  "mm_vision_select_layer": -2,
  "mm_vision_tower": "openai/clip-vit-large-patch14-336",
  "model_type": "llava_llama",
  "num_attention_heads": 40,
  "num_hidden_layers": 40,
  "num_key_value_heads": 40,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.37.2",
  "use_cache": false,
  "use_mm_proj": true,
  "vocab_size": 32000
}

LlavaConfig {
  "_name_or_path": "lmsys/vicuna-13b-v1.5",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "down_sample": false,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "mm_hidden_size": 1024,
  "mm_patch_merge_type": "flat",
  "mm_projector_type": "mlp2x_gelu",
  "mm_vision_select_feature": "patch",
  "mm_vision_select_layer": -2,
  "mm_vision_tower": "openai/clip-vit-large-patch14-336",
  "model_type": "llava_llama",
  "num_attention_heads": 40,
  "num_hidden_layers": 40,
  "num_key_value_heads": 40,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.37.2",
  "use_cache": false,
  "use_mm_proj": true,
  "vocab_size": 32000
}

build projector config LlavaConfig {
  "_name_or_path": "lmsys/vicuna-13b-v1.5",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "down_sample": false,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "mm_hidden_size": 1024,
  "mm_patch_merge_type": "flat",
  "mm_projector_type": "mlp2x_gelu",
  "mm_vision_select_feature": "patch",
  "mm_vision_select_layer": -2,
  "mm_vision_tower": "openai/clip-vit-large-patch14-336",
  "model_type": "llava_llama",
  "num_attention_heads": 40,
  "num_hidden_layers": 40,
  "num_key_value_heads": 40,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.37.2",
  "use_cache": false,
  "use_mm_proj": true,
  "vocab_size": 32000
}

build projector config LlavaConfig {
  "_name_or_path": "lmsys/vicuna-13b-v1.5",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "down_sample": false,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "mm_hidden_size": 1024,
  "mm_patch_merge_type": "flat",
  "mm_projector_type": "mlp2x_gelu",
  "mm_vision_select_feature": "patch",
  "mm_vision_select_layer": -2,
  "mm_vision_tower": "openai/clip-vit-large-patch14-336",
  "model_type": "llava_llama",
  "num_attention_heads": 40,
  "num_hidden_layers": 40,
  "num_key_value_heads": 40,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.37.2",
  "use_cache": false,
  "use_mm_proj": true,
  "vocab_size": 32000
}

Formatting inputs...Skip in lazy mode
Parameter Offload: Total persistent parameters: 749568 in 328 params
trainer compute loss
trainer compute loss
trainer compute loss
trainer compute loss
prepare_input_labels torch.Size([8, 1274]) torch.Size([8, 1274, 5120])
forward
input_ids None
input_embeds tensor([[[ 0.0050,  0.0046, -0.0016,  ...,  0.0047,  0.0112,  0.0003],
         [-0.0129, -0.0126,  0.0064,  ...,  0.0114,  0.0227, -0.0037],
         [ 0.0017,  0.0256, -0.0164,  ..., -0.0119,  0.0037, -0.0115],
         ...,
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],

        [[ 0.0050,  0.0046, -0.0016,  ...,  0.0047,  0.0112,  0.0003],
         [-0.0129, -0.0126,  0.0064,  ...,  0.0114,  0.0227, -0.0037],
         [ 0.0017,  0.0256, -0.0164,  ..., -0.0119,  0.0037, -0.0115],
         ...,
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],

        [[-0.0295, -0.0014,  0.0027,  ...,  0.0146,  0.0080, -0.0294],
         [-0.0173, -0.0110, -0.0203,  ...,  0.0130,  0.0239, -0.0190],
         [-0.0121,  0.0078, -0.0027,  ..., -0.0073,  0.0273, -0.0001],
         ...,
         [ 0.0049,  0.0154,  0.0054,  ...,  0.0513,  0.0417,  0.0118],
         [ 0.0165, -0.0237, -0.0010,  ...,  0.0029, -0.0082,  0.0228],
         [-0.0249, -0.0061,  0.0052,  ...,  0.0391, -0.0019, -0.0050]],

        ...,

        [[ 0.0245,  0.0226, -0.0032,  ...,  0.0096,  0.0276, -0.0075],
         [-0.0136,  0.0004, -0.0227,  ..., -0.0325,  0.0057, -0.0080],
         [ 0.0300,  0.0306, -0.0074,  ...,  0.0253,  0.0032,  0.0003],
         ...,
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],

        [[ 0.0245,  0.0226, -0.0032,  ...,  0.0096,  0.0276, -0.0075],
         [-0.0136,  0.0004, -0.0227,  ..., -0.0325,  0.0057, -0.0080],
         [-0.0240,  0.0126, -0.0070,  ..., -0.0228,  0.0251, -0.0099],
         ...,
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],

        [[ 0.0245,  0.0226, -0.0032,  ...,  0.0096,  0.0276, -0.0075],
         [-0.0136,  0.0004, -0.0227,  ..., -0.0325,  0.0057, -0.0080],
         [-0.0240,  0.0126, -0.0070,  ..., -0.0228,  0.0251, -0.0099],
         ...,
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],
       device='cuda:2', dtype=torch.bfloat16, grad_fn=<StackBackward0>)
labels tensor([[ -100,  -100,  -100,  ...,  -100,  -100,  -100],
        [ -100,  -100,  -100,  ...,  -100,  -100,  -100],
        [ -100,  -100,  -100,  ..., 30366, 30267,     2],
        ...,
        [ -100,  -100,  -100,  ...,  -100,  -100,  -100],
        [ -100,  -100,  -100,  ...,  -100,  -100,  -100],
        [ -100,  -100,  -100,  ...,  -100,  -100,  -100]], device='cuda:2')
llama input_ids None
llama input_embeds tensor([[[ 0.0050,  0.0046, -0.0016,  ...,  0.0047,  0.0112,  0.0003],
         [-0.0129, -0.0126,  0.0064,  ...,  0.0114,  0.0227, -0.0037],
         [ 0.0017,  0.0256, -0.0164,  ..., -0.0119,  0.0037, -0.0115],
         ...,
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],

        [[ 0.0050,  0.0046, -0.0016,  ...,  0.0047,  0.0112,  0.0003],
         [-0.0129, -0.0126,  0.0064,  ...,  0.0114,  0.0227, -0.0037],
         [ 0.0017,  0.0256, -0.0164,  ..., -0.0119,  0.0037, -0.0115],
         ...,
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],

        [[-0.0295, -0.0014,  0.0027,  ...,  0.0146,  0.0080, -0.0294],
         [-0.0173, -0.0110, -0.0203,  ...,  0.0130,  0.0239, -0.0190],
         [-0.0121,  0.0078, -0.0027,  ..., -0.0073,  0.0273, -0.0001],
         ...,
         [ 0.0049,  0.0154,  0.0054,  ...,  0.0513,  0.0417,  0.0118],
         [ 0.0165, -0.0237, -0.0010,  ...,  0.0029, -0.0082,  0.0228],
         [-0.0249, -0.0061,  0.0052,  ...,  0.0391, -0.0019, -0.0050]],

        ...,

        [[ 0.0245,  0.0226, -0.0032,  ...,  0.0096,  0.0276, -0.0075],
         [-0.0136,  0.0004, -0.0227,  ..., -0.0325,  0.0057, -0.0080],
         [ 0.0300,  0.0306, -0.0074,  ...,  0.0253,  0.0032,  0.0003],
         ...,
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],

        [[ 0.0245,  0.0226, -0.0032,  ...,  0.0096,  0.0276, -0.0075],
         [-0.0136,  0.0004, -0.0227,  ..., -0.0325,  0.0057, -0.0080],
         [-0.0240,  0.0126, -0.0070,  ..., -0.0228,  0.0251, -0.0099],
         ...,
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],

        [[ 0.0245,  0.0226, -0.0032,  ...,  0.0096,  0.0276, -0.0075],
         [-0.0136,  0.0004, -0.0227,  ..., -0.0325,  0.0057, -0.0080],
         [-0.0240,  0.0126, -0.0070,  ..., -0.0228,  0.0251, -0.0099],
         ...,
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],
       device='cuda:2', dtype=torch.bfloat16, grad_fn=<StackBackward0>)
llama labels tensor([[ -100,  -100,  -100,  ...,  -100,  -100,  -100],
        [ -100,  -100,  -100,  ...,  -100,  -100,  -100],
        [ -100,  -100,  -100,  ..., 30366, 30267,     2],
        ...,
        [ -100,  -100,  -100,  ...,  -100,  -100,  -100],
        [ -100,  -100,  -100,  ...,  -100,  -100,  -100],
        [ -100,  -100,  -100,  ...,  -100,  -100,  -100]], device='cuda:2')
llama return dict True
1
2
3
4
5
6
7
8
9
10
hidden states None
grad check
functools.partial(<function checkpoint at 0x14897c18d000>)
[2024-03-11 23:40:18,557] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3976858
[2024-03-11 23:40:18,782] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3976858
[2024-03-11 23:40:19,362] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3976859
[2024-03-11 23:40:20,010] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3976860
[2024-03-11 23:40:20,741] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3976861
[2024-03-11 23:40:21,416] [INFO] [launch.py:324:sigkill_handler] Main process received SIGINT, exiting
